{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/railanderreis/previsaoTempo/blob/main/previsao_tempo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580dd39c-f066-4c32-af47-648eb71f5918",
      "metadata": {
        "tags": [],
        "id": "580dd39c-f066-4c32-af47-648eb71f5918"
      },
      "source": [
        "# Desafio: Consumo de Dados para Previsão do Tempo das Cidades do Vale do Paraíba.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Avaliar conhecimentos nas linguagens Python e SQL e na engine de processamento Apache Spark.\n",
        "\n",
        "## Descrição\n",
        "\n",
        "Neste desafio, você desenvolverá um notebook que será responsável por extrair dados de previsão do tempo das cidades do Vale do Paraíba, região onde se localiza a Dataside. Para consultar todas as cidades dessa região, utilizaremos a API do IBGE. No caso, basta realizar uma requisição HTTP com o método GET, utilizando a URL abaixo:\n",
        "\n",
        "```\n",
        "https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes/3513/municipios\n",
        "```\n",
        "\n",
        "Com esses dados, gerar um data frame e a partir dele uma temp view. Ex: \"cities\"\n",
        "\n",
        "Utilizando os nomes das cidades, deverão ser consultados os dados de previsão de tempo para cada cidade. Para realizar essa consulta, poderá ser utilizada qualquer uma das APIs informadas no link abaixo.\n",
        "\n",
        "[Public APIs - Wather](https://github.com/public-apis/public-apis#weather)\n",
        "\n",
        "Obs.: Para algumas, pode ser necessário cadastrar-se para acessar sua API Key. Mas nenhuma delas deve precisar cadastrar cartão de crédito ou adicionar qualquer valor monetário para utilizar. Caso alguma solicite, basta optar por outra.\n",
        "\n",
        "Com os dados consultados, gerar um data frame e partir dele outra temp view. Ex: \"forecasts\"\n",
        "\n",
        "Com as temp views geradas, utilizar Spark SQL para criar queries e gerar data frames das seguintes tabelas:\n",
        "\n",
        "- Tabela 1: dados de previsão do tempo para os próximos cinco dias, para cada data e cidade consultadas. As colunas dessa tabela serão:\n",
        "    - Cidade\n",
        "    - CodigoDaCidade\n",
        "    - Data\n",
        "    - Regiao\n",
        "    - Pais\n",
        "    - Latitude\n",
        "    - Longigute\n",
        "    - TemperaturaMaxima\n",
        "    - TemperaturaMinima\n",
        "    - TemperaturaMedia\n",
        "    - VaiChover\n",
        "    - ChanceDeChuva\n",
        "    - CondicaoDoTempo\n",
        "    - NascerDoSol\n",
        "    - PorDoSol\n",
        "    - VelocidadeMaximaDoVento\n",
        "    \n",
        "    Obs.: Os valores da coluna \"VaiChover\" deverá ser \"Sim\" ou \"Não\". E a coluna \"CodigoDaCidade\" é o ID retornado junto com os nomes da cidades na API do IBGE.\n",
        "    Obs.: Dependendo da API utilizada, algumas colunas podem não existir e ficarão em branco. Você deve optar por uma API que traga o maior número de informações possível.\n",
        "\n",
        "- Tabela 2: quantidade de dias com chuva e sem chuva para os dias consultados, para cada data consultada. Colunas:\n",
        "    - Cidade\n",
        "    - QtdDiasVaiChover\n",
        "    - QtdDiasNaoVaiChover\n",
        "    - TotalDiasMapeados\n",
        "\n",
        "Essas tabelas deverão ser exportadas em formado CSV e entregue no final do desafio.\n",
        "\n",
        "## To Do\n",
        "\n",
        "[ ] - Consultar municípios do Vale do Paraíba, gerar um data frame e criar uma temp view com esses dados.\n",
        "[ ] - Consultar dados do tempo para cada município, gerar um data frame e criar uma outra temp view.\n",
        "[ ] - Utilizar Spark SQL para gerar os data frames das Tabelas 1 e 2.\n",
        "[ ] - Exportar os data frames para CSV.\n",
        "\n",
        "## Atenção\n",
        "\n",
        "- Existe um limite de requisições de 10000 requests por conta cadastrada na m3o.\n",
        "- Essa API pode retornar cidades de outras regiões que possuem nome semelhante a alguma cidade do Vale do Paraiba. Pode mantê-las ou filtrar para gerar as tabelas apenas com dados de Regiao = Sao Paulo. Fica a seu critério.\n",
        "\n",
        "## Entregando o desafio\n",
        "\n",
        "Concluindo todos os passos informados em To Do, basta salvar o arquivo .ipynb do notebook e enviar para a Dataside juntamente com os CSVs das duas tabelas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o PySpark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "GS57-yjTl2ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a355fe58-c3c1-4116-a4cb-80b1a5a79944"
      },
      "id": "GS57-yjTl2ga",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=932409109c438c405572e8c9c32f76dac9ca9887f142258fc1a3aae80859cd43\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "bWoDs4anmb9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6385ee2b-d9c0-45b1-9479-688612e7c2a0"
      },
      "id": "bWoDs4anmb9f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cb6c40-3073-43b0-b7f4-95e7c9dd8245",
      "metadata": {
        "tags": [],
        "id": "a1cb6c40-3073-43b0-b7f4-95e7c9dd8245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcbfdc1-5b0f-4860-ba1a-2c5c9078e2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
          ]
        }
      ],
      "source": [
        "# Importando a Lib\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "import requests\n",
        "import json\n",
        "import unidecode\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Ignorando avisos\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[1]\") \\\n",
        "      .appName(\"SparkByExamples.com\") \\\n",
        "      .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estou utilizando a api do do site open weather map."
      ],
      "metadata": {
        "id": "NI_r9Vc3Xh5i"
      },
      "id": "NI_r9Vc3Xh5i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscar cidades do Vale do Paraíba\n",
        "params = {\"accept\": \"application/json\"}\n",
        "url = \"https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes/3513/municipios\"\n",
        "data = requests.get(url, params=params).json()\n",
        "\n",
        "os.makedirs('./data/raw/municipios/', exist_ok=True)\n",
        "save_path ='data/raw/municipios/municipios.json'\n",
        "with open(save_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "df = spark.read.option('multiline', 'true').json('data/raw/municipios/municipios.json')\n",
        "df_municipios = df.select(\n",
        "    col(\"id\").alias(\"id_cidade\"),\n",
        "    col(\"nome\").alias(\"nome_cidade\"),\n",
        "    col(\"microrregiao.id\").alias(\"id_microrregiao\"),\n",
        "    col(\"microrregiao.nome\").alias(\"nome_microrregiao\"),\n",
        "    col(\"microrregiao.mesorregiao.id\").alias(\"id_mesorregiao\"),\n",
        "    col(\"microrregiao.mesorregiao.nome\").alias(\"nome_mesorregiao\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.id\").alias(\"id_UF\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.sigla\").alias(\"sigla_UF\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.nome\").alias(\"nome_UF\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.regiao.id\").alias(\"id_regiao\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.regiao.sigla\").alias(\"sigla_regiao\"),\n",
        "    col(\"microrregiao.mesorregiao.UF.regiao.nome\").alias(\"nome_regiao\"))\n",
        "\n",
        "df_municipios.createOrReplaceTempView(\"municipios\")\n",
        "\n",
        "cities = df_municipios.select('nome_cidade') \\\n",
        "             .rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Definindo as variaveis\n",
        "state = \"SP\"\n",
        "country_code = \"BRA\"\n",
        "api_key = '52bf0898f7a845d2a533e2394ab7c9a9'\n",
        "\n",
        "# Realizar download dos arquivos\n",
        "for i in cities:\n",
        "  url_api = f\"http://api.openweathermap.org/data/2.5/forecast?q={i},BR-{state},{country_code}&appid={api_key}&lang=pt_br\"\n",
        "  data = requests.get(url_api, params=params).json()\n",
        "\n",
        "  os.makedirs('./data/raw/cities/', exist_ok=True)\n",
        "\n",
        "  save_path = f'data/raw/cities/{i}.json'\n",
        "  with open(save_path, 'w', encoding='utf-8') as f:\n",
        "      json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Criar data frame com as cidades\n",
        "df_cities = spark.read.option('multiline', 'true').json('data/raw/cities/*.json')\n",
        "\n",
        "df_cities = df_cities.select(\n",
        "            col(\"city.id\").alias(\"idCidade\"),\n",
        "            col(\"city.name\").alias(\"nomeCidade\"),\n",
        "            col(\"list\").alias(\"listao\"),\n",
        "            col(\"city.country\").alias(\"pais\"),\n",
        "            col(\"city.coord.lat\").alias(\"latitude\"),\n",
        "            col(\"city.coord.lon\").alias(\"longetitude\"),\n",
        "            col(\"city.sunrise\").alias(\"nascerDoSol\"),\n",
        "            col(\"city.sunset\").alias(\"porDoSol\"))\n",
        "\n",
        "df_lista = df_cities.select('*',explode(df_cities.listao).alias(\"lista\")) \\\n",
        "                          .drop(\"listao\")\n",
        "\n",
        "df_lista = df_lista.select('*',explode(df_lista.lista.weather).alias(\"tempoLista\"))\n",
        "\n",
        "\n",
        "# Essa parte tive que usa o pandas para poder remover os acentos da coluna \"nomeCidade\".\n",
        "# Pois a api do IBGE tras os nomes das cidades sem acento.\n",
        "# OBS: poderia ter usado a \"API do Pandas - spark\" mas da forma que usei de encode ainda nao esta implementada, por isso fiz dessa forma.\n",
        "df_pd = df_lista.toPandas()\n",
        "df_pd.nomeCidade = df_pd.nomeCidade.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "df_lista_sp = spark.createDataFrame(df_pd)\n",
        "# --------------------------------------------\n",
        "\n",
        "df_lista_final = df_lista_sp.select(\n",
        "            col(\"idCidade\"),\n",
        "            col(\"nomeCidade\"),\n",
        "            col(\"pais\"),\n",
        "            col(\"latitude\"),\n",
        "            col(\"longetitude\"),\n",
        "            col(\"nascerDoSol\"),\n",
        "            col(\"porDoSol\"),\n",
        "            col(\"lista.dt_txt\").alias(\"data\"),\n",
        "            col(\"lista.main.temp\").alias(\"tempMed\"),\n",
        "            col(\"lista.main.temp\").alias(\"tempMax\"),\n",
        "            col(\"lista.main.temp\").alias(\"tempMin\"),\n",
        "            col(\"lista.clouds.all\").alias(\"chanceChuva\"),\n",
        "            col(\"tempoLista.description\").alias(\"tempo\"),\n",
        "            col(\"tempoLista.main\").alias(\"vaiChover\"),\n",
        "            col(\"lista.wind.speed\").alias(\"velMaxVento\"))\n",
        "\n",
        "# Criar view com as cidades\n",
        "df_lista_final.createOrReplaceTempView(\"cities_api\")\n"
      ],
      "metadata": {
        "id": "jaC3ec7CFbfl"
      },
      "id": "jaC3ec7CFbfl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a40a6f-d5f1-4524-9d0b-d1e6e24dfbfa",
      "metadata": {
        "tags": [],
        "id": "c4a40a6f-d5f1-4524-9d0b-d1e6e24dfbfa"
      },
      "outputs": [],
      "source": [
        "# Buscar previsão do tempo para as cidades\n",
        "df_join = spark.sql(\"\"\"SELECT * FROM cities_api\n",
        "             FULL OUTER JOIN municipios ON cities_api.nomeCidade = municipios.nome_cidade\n",
        "             ORDER BY municipios.id_cidade\"\"\")\n",
        "\n",
        "# Criar data frame com as previsões\n",
        "df_join = df_join.drop(\"idCidade\",\"nome_cidade\",\"nome_mesorregiao\",\"id_mesorregiao\", \"id_microrregiao\",\"sigla_regiao\",\"nome_microrregiao\",\"nome_UF\",\"id_regiao\",\"siga_regiao\")\n",
        "\n",
        "df_silver = df_join.withColumn(\"vaiChover\", \n",
        "                                  when((col(\"vaiChover\") == \"Clear\" ) | (col(\"vaiChover\") == \"Few clouds\") | (col(\"vaiChover\") == \"Scattered clouds\") | (col(\"vaiChover\") == \"Broken clouds\") | (col(\"vaiChover\") == \"Snow\") | (col(\"vaiChover\") == \"Mist\") | (col(\"vaiChover\") == \"Clouds\"),\"Não\")\n",
        "                                  .when((col(\"vaiChover\") == \"Shower rain\") | (col(\"vaiChover\") == \"Rain\") | (col(\"vaiChover\") == \"Thunderstorm\"), \"Sim\"))\n",
        "\n",
        "df_silver_transf = df_silver.withColumn(\"tempMax\", concat((round((col(\"tempMax\")-273.15),2)).cast(\"double\"),lit('°C'))) \\\n",
        "                  .withColumn(\"tempMin\", concat((round((col(\"tempMin\")-273.15),2)).cast(\"double\"),lit('°C'))) \\\n",
        "                  .withColumn(\"tempMed\", concat((round((col(\"tempMed\")-273.15),2)).cast(\"double\"),lit('°C'))) \\\n",
        "                  .withColumn(\"chanceChuva\", concat(((col(\"chanceChuva\"))).cast(\"double\"),lit('%'))) \\\n",
        "                  .withColumn(\"nascerDoSol\",from_unixtime(col(\"nascerDoSol\"))) \\\n",
        "                  .withColumn(\"porDoSol\",from_unixtime(col(\"porDoSol\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc2a925-c707-46f0-a2e2-e0e0164a7312",
      "metadata": {
        "id": "bbc2a925-c707-46f0-a2e2-e0e0164a7312"
      },
      "outputs": [],
      "source": [
        "# Criar DF da Tabela 1\n",
        "\n",
        "df_gold = df_silver_transf[['id_cidade', 'nomeCidade', 'data','nome_regiao','pais','latitude','longetitude','tempMax','tempMin','tempMed','vaiChover','chanceChuva','nascerDoSol','porDoSol','velMaxVento']]\n",
        "\n",
        "# Criar view com as previsões\n",
        "df_gold.createOrReplaceTempView(\"cities_previsao\")\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bab3315-f50b-4269-8823-ccfda0fefbfe",
      "metadata": {
        "id": "3bab3315-f50b-4269-8823-ccfda0fefbfe"
      },
      "outputs": [],
      "source": [
        "# Criar DF da Tabela 2\n",
        "\n",
        "df_gold_sql = spark.sql(\"\"\"SELECT nomeCidade AS cidade,COUNT(data) AS totalDiasMapeados,\n",
        "             COUNT(CASE WHEN vaiChover = 'Não' THEN 'id_cidade' ELSE NULL END) AS QtdDiasNaoChover,\n",
        "             COUNT(CASE WHEN vaiChover = 'Sim' THEN 'id_cidade' ELSE NULL END) AS QtdDiasVaiChover \n",
        "             FROM cities_previsao\n",
        "             WHERE data LIKE '%12%'\n",
        "             GROUP BY nomeCidade\n",
        "             ORDER BY nomeCidade ASC\n",
        "             \"\"\")\n",
        "\n",
        "# OBS: utilizei o 'WHERE LIKE %12%', pois a API utilizada me retornava 5 dias de consultas\n",
        "#      a cada 3 horas e como nao conseguir fazer um query mais adequada eu deixei do jeito que eu conseguir\n",
        "#      trazer solicitado.\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ff378b-4c24-47dc-aba1-742211cd385d",
      "metadata": {
        "id": "c1ff378b-4c24-47dc-aba1-742211cd385d"
      },
      "outputs": [],
      "source": [
        "# Exportar CSVs\n",
        "\n",
        "# CSV dos dados formatados\n",
        "df_gold.write.option(\"header\",True) \\\n",
        "       .option('sep', ',') \\\n",
        "   .csv('data/gold/municipios.csv')\n",
        "\n",
        "# CSV previsao do tempo.\n",
        "df_gold_sql.write.option(\"header\",True) \\\n",
        "              .option('sep', ',') \\\n",
        "              .option('mode','append') \\\n",
        "              .csv('./data/gold/previsaoTempo.csv')\n",
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}